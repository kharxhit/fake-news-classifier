import nltk
import numpy as np
import pandas as pd

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.optimizers import Adam

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('punkt')

# Load the data
data = pd.read_csv('news_data.csv')  # Replace 'news_data.csv' with your dataset filename

# Preprocessing
stopwords_set = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase
    tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]  # Stemming and remove non-alphabetic tokens
    tokens = [token for token in tokens if token not in stopwords_set]  # Remove stopwords
    return ' '.join(tokens)

data['processed_text'] = data['text'].apply(preprocess_text)

# One-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
labels = onehot_encoder.fit_transform(np.array(data['label']).reshape(-1, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['processed_text'], labels, test_size=0.2, random_state=42)

# Tokenization and Padding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train_tokens = tokenizer.texts_to_sequences(X_train)
X_test_tokens = tokenizer.texts_to_sequences(X_test)

vocab_size = len(tokenizer.word_index) + 1
max_length = max([len(tokens) for tokens in X_train_tokens])

X_train_padded = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')
X_test_padded = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')

# Build the LSTM model
model = Sequential()
model.add(Embedding(vocab_size, 100, input_length=max_length))
model.add(LSTM(128))
model.add(Dense(2, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Train the model
model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_test_padded, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Accuracy: {accuracy * 100}%")
